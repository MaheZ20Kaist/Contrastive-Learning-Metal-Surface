{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EH5MZH-Ws5Y",
        "outputId": "3de02af2-5af9-472e-854e-755fc6b1e2ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-22 18:01:20.663343: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-22 18:01:21.330914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/home/khubayeeb_k/.local/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tensorflow_addons as tfa\n",
        "import time\n",
        "import math\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyQso4utWs5Z"
      },
      "outputs": [],
      "source": [
        "class_types = [\"Cr\", \"Sc\", \"In\", \"PS\", \"RS\", \"Pa\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_wRv_ejWs5Z",
        "outputId": "ac0802a5-fd03-4373-ea5d-bd09c6c19aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1266 14 1 4\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "os.chdir(\"neu-metal-surface/train/\")\n",
        "\"\"\"\n",
        "Cr - 0\n",
        "Sc - 1\n",
        "In - 2\n",
        "PS - 3\n",
        "RS - 4\n",
        "Pa - 5\n",
        "\"\"\"\n",
        "ls_Cr = glob.glob(\"Cr*.bmp\")\n",
        "ls_Sc = glob.glob(\"Sc*.bmp\")\n",
        "ls_In = glob.glob(\"In*.bmp\")\n",
        "ls_PS = glob.glob(\"PS*.bmp\")\n",
        "ls_RS = glob.glob(\"RS*.bmp\")\n",
        "ls_Pa = glob.glob(\"Pa*.bmp\")\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "IMAGE_SIZE = (224, 224)\n",
        "image_size = 224\n",
        "patch_size = 16\n",
        "UNLABLED_DATA_SIZE = 18974\n",
        "LABLED_DATA_SIZE_TRAIN = 1283\n",
        "LABLED_DATA_SIZE_TEST = 517\n",
        "\n",
        "STEPS_PER_EPOCH = (UNLABLED_DATA_SIZE + LABLED_DATA_SIZE_TRAIN) // BATCH_SIZE\n",
        "UNLABELED_BATCH_SIZE = UNLABLED_DATA_SIZE // STEPS_PER_EPOCH\n",
        "LABLED_BATCH_SIZE_TRAIN = LABLED_DATA_SIZE_TRAIN // STEPS_PER_EPOCH\n",
        "LABLED_BATCH_SIZE_TEST = LABLED_DATA_SIZE_TEST//(STEPS_PER_EPOCH//10)\n",
        "\n",
        "print(STEPS_PER_EPOCH, UNLABELED_BATCH_SIZE,\n",
        "      LABLED_BATCH_SIZE_TRAIN, LABLED_BATCH_SIZE_TEST)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "\n",
        "def decode_jpg(img_name):\n",
        "    img = tf.io.read_file(img_name)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img\n",
        "\n",
        "\n",
        "def preprocessing_jpg(img_name, unlabled_img_labels):\n",
        "    return decode_jpg(img_name), unlabled_img_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ9kFfhsWs5a"
      },
      "outputs": [],
      "source": [
        "train_image_path, train_image_labels = ls_Cr + ls_Sc + ls_In + ls_PS + ls_RS + \\\n",
        "    ls_Pa, [0]*len(ls_Cr) + [1]*len(ls_Sc) + [2]*len(ls_In) + \\\n",
        "    [3]*len(ls_PS) + [4]*len(ls_RS) + [5]*len(ls_Pa)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KP3PJlEWWs5a",
        "outputId": "8aa85c84-ab6a-4ce3-d244-4bcbd8f525f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_image_path = [os.path.join(\n",
        "    \"neu-metal-surface\", \"train\", path) for path in train_image_path]\n",
        "len(train_image_labels) == len(train_image_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV_PqktVWs5b"
      },
      "outputs": [],
      "source": [
        "def decode_bmp(img_name):\n",
        "    img = tf.io.read_file(img_name)\n",
        "    img = tf.image.decode_bmp(img, channels=0)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img\n",
        "\n",
        "\n",
        "def preprocessing_bmp(img_name, labels):\n",
        "    return decode_bmp(img_name), labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr5vHCNwWs5b",
        "outputId": "5f3ea412-4a9f-47cd-df67-b7c0e1704291"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-22 18:01:22.139293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-22 18:01:22.159201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-22 18:01:22.159441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-22 18:01:22.161457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-22 18:01:22.161703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-22 18:01:22.161895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-22 18:01:22.218859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-22 18:01:22.219027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-22 18:01:22.219156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-22 18:01:22.219255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1303 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "labeled_train_data = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_image_path, train_image_labels))\n",
        "labeled_train_data = labeled_train_data.map(preprocessing_bmp).prefetch(12)\n",
        "labeled_train_data = labeled_train_data.shuffle(LABLED_DATA_SIZE_TRAIN)\n",
        "labeled_train_data = labeled_train_data.batch(\n",
        "    LABLED_BATCH_SIZE_TRAIN).prefetch(AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRL5jr54Ws5b"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"../test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1d-kz9LWs5b"
      },
      "outputs": [],
      "source": [
        "ls_Cr = glob.glob(\"Cr*.bmp\")\n",
        "ls_Sc = glob.glob(\"Sc*.bmp\")\n",
        "ls_In = glob.glob(\"In*.bmp\")\n",
        "ls_PS = glob.glob(\"PS*.bmp\")\n",
        "ls_RS = glob.glob(\"RS*.bmp\")\n",
        "ls_Pa = glob.glob(\"Pa*.bmp\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnMbw-QOWs5b"
      },
      "outputs": [],
      "source": [
        "test_image_path, test_image_labels = ls_Cr + ls_Sc + ls_In + ls_PS + ls_RS + \\\n",
        "    ls_Pa, [0]*len(ls_Cr) + [1]*len(ls_Sc) + [2]*len(ls_In) + \\\n",
        "    [3]*len(ls_PS) + [4]*len(ls_RS) + [5]*len(ls_Pa)\n",
        "test_image_path = [os.path.join(\"neu-metal-surface\", \"test\", path)\n",
        "                   for path in test_image_path]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT0W-6fNWs5c"
      },
      "outputs": [],
      "source": [
        "labeled_test_data = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_image_path, test_image_labels))\n",
        "labeled_test_data = labeled_test_data.map(preprocessing_bmp)\n",
        "labeled_test_data = labeled_test_data.shuffle(LABLED_DATA_SIZE_TEST)\n",
        "labeled_test_data = labeled_test_data.batch(\n",
        "    LABLED_BATCH_SIZE_TEST).prefetch(AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zxlAHw8Ws5c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.join(\"..\", \"..\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDgFglw3Ws5c",
        "outputId": "34474b15-a105-4c3a-dd2d-6f746598ca34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1728, 224, 224, 1) (72, 224, 224, 1)\n",
            "(1728, 1) (72, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from skimage.transform import resize\n",
        "from skimage import io\n",
        "X_train = []\n",
        "X_test = []\n",
        "for path in train_image_path:\n",
        "    # img = tf.convert_to_tensor(io.imread(path), dtype=tf.float32)\n",
        "    img = resize(io.imread(path), output_shape=[*IMAGE_SIZE])\n",
        "    img = np.reshape(img, newshape=(*img.shape, 1))\n",
        "    X_train.append(img)\n",
        "for path in test_image_path:\n",
        "    # img = tf.convert_to_tensor(io.imread(path), dtype=tf.float32)\n",
        "    img = resize(io.imread(path), output_shape=[*IMAGE_SIZE])\n",
        "    img = np.reshape(img, newshape=(*img.shape, 1))\n",
        "    X_test.append(img)\n",
        "X_train, X_test = np.array(X_train), np.array(X_test)\n",
        "\n",
        "# y_train, y_test = np.array(train_image_labels), np.array(test_image_labels)\n",
        "\n",
        "# enc = OneHotEncoder(handle_unknown='ignore')\n",
        "# y_train = enc.fit_transform(np.expand_dims(\n",
        "#     np.array(train_image_labels), axis=-1))\n",
        "# y_test = enc.transform(np.expand_dims(np.array(test_image_labels), axis=-1))\n",
        "\n",
        "\n",
        "y_train = np.expand_dims(\n",
        "    np.array(train_image_labels), axis=-1)\n",
        "y_test = np.expand_dims(np.array(test_image_labels), axis=-1)\n",
        "\n",
        "\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOStwv6lWs5c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8yiYFO_Ws5c"
      },
      "outputs": [],
      "source": [
        "# define the number of classes\n",
        "NUM_CLASSES = 6\n",
        "# Use Kaggle_02 - Mem_MAX\n",
        "\n",
        "# input_shape = (224, 224, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGOresatWs5c"
      },
      "outputs": [],
      "source": [
        "# DATA\n",
        "BUFFER_SIZE = 512\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# AUGMENTATION\n",
        "IMAGE_SIZE = 224\n",
        "PATCH_SIZE = 16\n",
        "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 0.0001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "\n",
        "# TRAINING\n",
        "EPOCHS = 50\n",
        "\n",
        "# ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "TRANSFORMER_LAYERS = 6\n",
        "PROJECTION_DIM = 64\n",
        "NUM_HEADS = 4\n",
        "TRANSFORMER_UNITS = [\n",
        "    PROJECTION_DIM * 2,\n",
        "    PROJECTION_DIM,\n",
        "]\n",
        "MLP_HEAD_UNITS = [1024, 512]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgOmDD5BWs5d"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Wf5x-qQWs5d"
      },
      "outputs": [],
      "source": [
        "class ShiftedPatchTokenization(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_patches=NUM_PATCHES,\n",
        "        projection_dim=PROJECTION_DIM,\n",
        "        vanilla=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vanilla = vanilla  # Flag to swtich to vanilla patch extractor\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.half_patch = patch_size // 2\n",
        "        self.flatten_patches = layers.Reshape((num_patches, -1))\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
        "\n",
        "    def crop_shift_pad(self, images, mode):\n",
        "        # Build the diagonally shifted images\n",
        "        if mode == \"left-up\":\n",
        "            crop_height = self.half_patch\n",
        "            crop_width = self.half_patch\n",
        "            shift_height = 0\n",
        "            shift_width = 0\n",
        "        elif mode == \"left-down\":\n",
        "            crop_height = 0\n",
        "            crop_width = self.half_patch\n",
        "            shift_height = self.half_patch\n",
        "            shift_width = 0\n",
        "        elif mode == \"right-up\":\n",
        "            crop_height = self.half_patch\n",
        "            crop_width = 0\n",
        "            shift_height = 0\n",
        "            shift_width = self.half_patch\n",
        "        else:\n",
        "            crop_height = 0\n",
        "            crop_width = 0\n",
        "            shift_height = self.half_patch\n",
        "            shift_width = self.half_patch\n",
        "\n",
        "        # Crop the shifted images and pad them\n",
        "        crop = tf.image.crop_to_bounding_box(\n",
        "            images,\n",
        "            offset_height=crop_height,\n",
        "            offset_width=crop_width,\n",
        "            target_height=self.image_size - self.half_patch,\n",
        "            target_width=self.image_size - self.half_patch,\n",
        "        )\n",
        "        shift_pad = tf.image.pad_to_bounding_box(\n",
        "            crop,\n",
        "            offset_height=shift_height,\n",
        "            offset_width=shift_width,\n",
        "            target_height=self.image_size,\n",
        "            target_width=self.image_size,\n",
        "        )\n",
        "        return shift_pad\n",
        "\n",
        "    def call(self, images):\n",
        "        if not self.vanilla:\n",
        "            # Concat the shifted images with the original image\n",
        "            images = tf.concat(\n",
        "                [\n",
        "                    images,\n",
        "                    self.crop_shift_pad(images, mode=\"left-up\"),\n",
        "                    self.crop_shift_pad(images, mode=\"left-down\"),\n",
        "                    self.crop_shift_pad(images, mode=\"right-up\"),\n",
        "                    self.crop_shift_pad(images, mode=\"right-down\"),\n",
        "                ],\n",
        "                axis=-1,\n",
        "            )\n",
        "        # Patchify the images and flatten it\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        flat_patches = self.flatten_patches(patches)\n",
        "        if not self.vanilla:\n",
        "            # Layer normalize the flat patches and linearly project it\n",
        "            tokens = self.layer_norm(flat_patches)\n",
        "            tokens = self.projection(tokens)\n",
        "        else:\n",
        "            # Linearly project the flat patches\n",
        "            tokens = self.projection(flat_patches)\n",
        "        return (tokens, patches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOI-DOkmWs5d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4ztbsStWs5d"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(\n",
        "        self, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "\n",
        "    def call(self, encoded_patches):\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_patches = encoded_patches + encoded_positions\n",
        "        return encoded_patches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYgZAtMIWs5d"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # The trainable temperature term. The initial value is\n",
        "        # the square root of the key dimension.\n",
        "        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n",
        "\n",
        "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
        "        query = tf.multiply(query, 1.0 / self.tau)\n",
        "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
        "        attention_scores = self._masked_softmax(\n",
        "            attention_scores, attention_mask)\n",
        "        attention_scores_dropout = self._dropout_layer(\n",
        "            attention_scores, training=training\n",
        "        )\n",
        "        attention_output = tf.einsum(\n",
        "            self._combine_equation, attention_scores_dropout, value\n",
        "        )\n",
        "        return attention_output, attention_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnxCACVdWs5d"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "# Build the diagonal attention mask\n",
        "diag_attn_mask = 1 - tf.eye(NUM_PATCHES)\n",
        "diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8IqgwKIWs5d"
      },
      "outputs": [],
      "source": [
        "def create_vit_classifier(vanilla=False):\n",
        "    inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1))\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    (tokens, _) = ShiftedPatchTokenization(vanilla=vanilla)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder()(tokens)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(TRANSFORMER_LAYERS):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        if not vanilla:\n",
        "            attention_output = MultiHeadAttentionLSA(\n",
        "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
        "            )(x1, x1, attention_mask=diag_attn_mask)\n",
        "        else:\n",
        "            attention_output = layers.MultiHeadAttention(\n",
        "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
        "            )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=MLP_HEAD_UNITS,\n",
        "                   dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(NUM_CLASSES)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mstdlXt_Ws5e",
        "outputId": "0884d3cd-4f91-42b9-d8fe-fe1bddc65d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "  4/195 [..............................] - ETA: 4s - loss: 3.1557 - accuracy: 0.2188 - top-5-accuracy: 0.9375   "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-22 18:01:31.776895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "195/195 [==============================] - 11s 31ms/step - loss: 2.7908 - accuracy: 0.3537 - top-5-accuracy: 0.9280 - val_loss: 2.2525 - val_accuracy: 0.2428 - val_top-5-accuracy: 0.9711\n",
            "Epoch 2/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 1.8819 - accuracy: 0.4238 - top-5-accuracy: 0.9537 - val_loss: 2.0584 - val_accuracy: 0.3699 - val_top-5-accuracy: 0.9827\n",
            "Epoch 3/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 1.4188 - accuracy: 0.5170 - top-5-accuracy: 0.9711 - val_loss: 1.3684 - val_accuracy: 0.5491 - val_top-5-accuracy: 0.9827\n",
            "Epoch 4/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 1.0798 - accuracy: 0.6302 - top-5-accuracy: 0.9904 - val_loss: 0.8740 - val_accuracy: 0.6590 - val_top-5-accuracy: 0.9942\n",
            "Epoch 5/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.8935 - accuracy: 0.6875 - top-5-accuracy: 0.9949 - val_loss: 0.5162 - val_accuracy: 0.7919 - val_top-5-accuracy: 1.0000\n",
            "Epoch 6/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.6807 - accuracy: 0.7653 - top-5-accuracy: 0.9955 - val_loss: 0.2925 - val_accuracy: 0.8960 - val_top-5-accuracy: 1.0000\n",
            "Epoch 7/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.5697 - accuracy: 0.8116 - top-5-accuracy: 0.9981 - val_loss: 0.8609 - val_accuracy: 0.6416 - val_top-5-accuracy: 0.9827\n",
            "Epoch 8/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5142 - accuracy: 0.8322 - top-5-accuracy: 0.9974 - val_loss: 0.2505 - val_accuracy: 0.9133 - val_top-5-accuracy: 1.0000\n",
            "Epoch 9/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.4056 - accuracy: 0.8643 - top-5-accuracy: 1.0000 - val_loss: 0.7577 - val_accuracy: 0.7168 - val_top-5-accuracy: 0.9942\n",
            "Epoch 10/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.3927 - accuracy: 0.8701 - top-5-accuracy: 0.9994 - val_loss: 0.4681 - val_accuracy: 0.8150 - val_top-5-accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "195/195 [==============================] - 5s 26ms/step - loss: 0.3814 - accuracy: 0.8688 - top-5-accuracy: 1.0000 - val_loss: 0.1752 - val_accuracy: 0.9480 - val_top-5-accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.3258 - accuracy: 0.8785 - top-5-accuracy: 1.0000 - val_loss: 0.5073 - val_accuracy: 0.8208 - val_top-5-accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.3302 - accuracy: 0.9016 - top-5-accuracy: 1.0000 - val_loss: 0.5116 - val_accuracy: 0.8324 - val_top-5-accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.3146 - accuracy: 0.9068 - top-5-accuracy: 0.9987 - val_loss: 0.1290 - val_accuracy: 0.9653 - val_top-5-accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.3142 - accuracy: 0.9068 - top-5-accuracy: 1.0000 - val_loss: 0.1268 - val_accuracy: 0.9769 - val_top-5-accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "195/195 [==============================] - 5s 26ms/step - loss: 0.2720 - accuracy: 0.9158 - top-5-accuracy: 0.9994 - val_loss: 0.1550 - val_accuracy: 0.9653 - val_top-5-accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.2572 - accuracy: 0.9170 - top-5-accuracy: 1.0000 - val_loss: 0.3377 - val_accuracy: 0.9133 - val_top-5-accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.2250 - accuracy: 0.9312 - top-5-accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9538 - val_top-5-accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.2690 - accuracy: 0.9138 - top-5-accuracy: 1.0000 - val_loss: 0.2933 - val_accuracy: 0.9075 - val_top-5-accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.2098 - accuracy: 0.9312 - top-5-accuracy: 1.0000 - val_loss: 0.2852 - val_accuracy: 0.9075 - val_top-5-accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.2100 - accuracy: 0.9350 - top-5-accuracy: 1.0000 - val_loss: 0.3139 - val_accuracy: 0.9133 - val_top-5-accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.2928 - accuracy: 0.9080 - top-5-accuracy: 1.0000 - val_loss: 0.3108 - val_accuracy: 0.9133 - val_top-5-accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.2497 - accuracy: 0.9170 - top-5-accuracy: 0.9994 - val_loss: 0.1617 - val_accuracy: 0.9595 - val_top-5-accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.1733 - accuracy: 0.9408 - top-5-accuracy: 1.0000 - val_loss: 0.1948 - val_accuracy: 0.9480 - val_top-5-accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.2127 - accuracy: 0.9267 - top-5-accuracy: 1.0000 - val_loss: 0.5114 - val_accuracy: 0.8382 - val_top-5-accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.1669 - accuracy: 0.9460 - top-5-accuracy: 1.0000 - val_loss: 0.1449 - val_accuracy: 0.9595 - val_top-5-accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.1777 - accuracy: 0.9402 - top-5-accuracy: 1.0000 - val_loss: 0.2573 - val_accuracy: 0.9017 - val_top-5-accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.2099 - accuracy: 0.9286 - top-5-accuracy: 1.0000 - val_loss: 0.5246 - val_accuracy: 0.8208 - val_top-5-accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "195/195 [==============================] - 5s 26ms/step - loss: 0.1767 - accuracy: 0.9383 - top-5-accuracy: 1.0000 - val_loss: 0.3351 - val_accuracy: 0.9017 - val_top-5-accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.1724 - accuracy: 0.9460 - top-5-accuracy: 1.0000 - val_loss: 0.1948 - val_accuracy: 0.9480 - val_top-5-accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.2128 - accuracy: 0.9286 - top-5-accuracy: 1.0000 - val_loss: 0.0771 - val_accuracy: 0.9827 - val_top-5-accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.1950 - accuracy: 0.9350 - top-5-accuracy: 1.0000 - val_loss: 0.3579 - val_accuracy: 0.9133 - val_top-5-accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.1637 - accuracy: 0.9441 - top-5-accuracy: 1.0000 - val_loss: 0.3424 - val_accuracy: 0.9017 - val_top-5-accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.1793 - accuracy: 0.9408 - top-5-accuracy: 1.0000 - val_loss: 0.1657 - val_accuracy: 0.9306 - val_top-5-accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.1624 - accuracy: 0.9447 - top-5-accuracy: 1.0000 - val_loss: 0.1947 - val_accuracy: 0.9538 - val_top-5-accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.1529 - accuracy: 0.9447 - top-5-accuracy: 1.0000 - val_loss: 0.5987 - val_accuracy: 0.8035 - val_top-5-accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.1778 - accuracy: 0.9338 - top-5-accuracy: 1.0000 - val_loss: 0.1814 - val_accuracy: 0.9364 - val_top-5-accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.1359 - accuracy: 0.9543 - top-5-accuracy: 1.0000 - val_loss: 0.1477 - val_accuracy: 0.9480 - val_top-5-accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.1599 - accuracy: 0.9447 - top-5-accuracy: 1.0000 - val_loss: 0.1659 - val_accuracy: 0.9306 - val_top-5-accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.1790 - accuracy: 0.9363 - top-5-accuracy: 1.0000 - val_loss: 0.1608 - val_accuracy: 0.9364 - val_top-5-accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.1482 - accuracy: 0.9486 - top-5-accuracy: 1.0000 - val_loss: 0.2691 - val_accuracy: 0.9017 - val_top-5-accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.1861 - accuracy: 0.9402 - top-5-accuracy: 0.9994 - val_loss: 0.1761 - val_accuracy: 0.9364 - val_top-5-accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.1191 - accuracy: 0.9601 - top-5-accuracy: 1.0000 - val_loss: 0.3335 - val_accuracy: 0.9017 - val_top-5-accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.1353 - accuracy: 0.9569 - top-5-accuracy: 1.0000 - val_loss: 0.2205 - val_accuracy: 0.9191 - val_top-5-accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.1737 - accuracy: 0.9408 - top-5-accuracy: 1.0000 - val_loss: 0.0852 - val_accuracy: 0.9711 - val_top-5-accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.1368 - accuracy: 0.9614 - top-5-accuracy: 1.0000 - val_loss: 0.5686 - val_accuracy: 0.8555 - val_top-5-accuracy: 0.9884\n",
            "Epoch 47/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.1592 - accuracy: 0.9473 - top-5-accuracy: 1.0000 - val_loss: 0.2113 - val_accuracy: 0.9306 - val_top-5-accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.1954 - accuracy: 0.9363 - top-5-accuracy: 1.0000 - val_loss: 0.5221 - val_accuracy: 0.8439 - val_top-5-accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.1360 - accuracy: 0.9550 - top-5-accuracy: 1.0000 - val_loss: 0.2683 - val_accuracy: 0.9133 - val_top-5-accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.1253 - accuracy: 0.9576 - top-5-accuracy: 1.0000 - val_loss: 0.2183 - val_accuracy: 0.9364 - val_top-5-accuracy: 1.0000\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0507 - accuracy: 1.0000 - top-5-accuracy: 1.0000\n",
            "Test accuracy: 100.0%\n",
            "Test top 5 accuracy: 100.0%\n",
            "Epoch 1/50\n",
            "195/195 [==============================] - 12s 34ms/step - loss: 2.8123 - accuracy: 0.2855 - top-5-accuracy: 0.9190 - val_loss: 1.9919 - val_accuracy: 0.0925 - val_top-5-accuracy: 0.9306\n",
            "Epoch 2/50\n",
            "195/195 [==============================] - 6s 30ms/step - loss: 2.0719 - accuracy: 0.3151 - top-5-accuracy: 0.9286 - val_loss: 1.6804 - val_accuracy: 0.2428 - val_top-5-accuracy: 0.9942\n",
            "Epoch 3/50\n",
            "195/195 [==============================] - 6s 30ms/step - loss: 1.6939 - accuracy: 0.3743 - top-5-accuracy: 0.9543 - val_loss: 1.5797 - val_accuracy: 0.2890 - val_top-5-accuracy: 1.0000\n",
            "Epoch 4/50\n",
            "195/195 [==============================] - 6s 30ms/step - loss: 1.5164 - accuracy: 0.4232 - top-5-accuracy: 0.9659 - val_loss: 1.6469 - val_accuracy: 0.2428 - val_top-5-accuracy: 0.9653\n",
            "Epoch 5/50\n",
            "195/195 [==============================] - 6s 30ms/step - loss: 1.3808 - accuracy: 0.4759 - top-5-accuracy: 0.9711 - val_loss: 1.4436 - val_accuracy: 0.3179 - val_top-5-accuracy: 0.9942\n",
            "Epoch 6/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 1.2613 - accuracy: 0.5113 - top-5-accuracy: 0.9826 - val_loss: 1.3937 - val_accuracy: 0.4220 - val_top-5-accuracy: 0.9884\n",
            "Epoch 7/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 1.0888 - accuracy: 0.5711 - top-5-accuracy: 0.9942 - val_loss: 1.2294 - val_accuracy: 0.5549 - val_top-5-accuracy: 1.0000\n",
            "Epoch 8/50\n",
            "195/195 [==============================] - 5s 26ms/step - loss: 0.9967 - accuracy: 0.6289 - top-5-accuracy: 0.9910 - val_loss: 1.3177 - val_accuracy: 0.5434 - val_top-5-accuracy: 1.0000\n",
            "Epoch 9/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.9206 - accuracy: 0.6360 - top-5-accuracy: 0.9968 - val_loss: 0.8724 - val_accuracy: 0.7457 - val_top-5-accuracy: 1.0000\n",
            "Epoch 10/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.7903 - accuracy: 0.6977 - top-5-accuracy: 0.9981 - val_loss: 0.8578 - val_accuracy: 0.6879 - val_top-5-accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.7278 - accuracy: 0.7215 - top-5-accuracy: 0.9987 - val_loss: 0.6759 - val_accuracy: 0.7746 - val_top-5-accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.6527 - accuracy: 0.7395 - top-5-accuracy: 0.9987 - val_loss: 0.6345 - val_accuracy: 0.7688 - val_top-5-accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5427 - accuracy: 0.8129 - top-5-accuracy: 0.9987 - val_loss: 0.6144 - val_accuracy: 0.7457 - val_top-5-accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5573 - accuracy: 0.8026 - top-5-accuracy: 0.9987 - val_loss: 0.7001 - val_accuracy: 0.7803 - val_top-5-accuracy: 0.9942\n",
            "Epoch 15/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4675 - accuracy: 0.8315 - top-5-accuracy: 0.9974 - val_loss: 0.5196 - val_accuracy: 0.8439 - val_top-5-accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.4264 - accuracy: 0.8572 - top-5-accuracy: 1.0000 - val_loss: 0.5788 - val_accuracy: 0.7803 - val_top-5-accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "195/195 [==============================] - 6s 30ms/step - loss: 0.3676 - accuracy: 0.8688 - top-5-accuracy: 0.9981 - val_loss: 0.3770 - val_accuracy: 0.8902 - val_top-5-accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.3485 - accuracy: 0.8727 - top-5-accuracy: 0.9987 - val_loss: 0.4201 - val_accuracy: 0.9017 - val_top-5-accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.3447 - accuracy: 0.8810 - top-5-accuracy: 0.9994 - val_loss: 0.2379 - val_accuracy: 0.9249 - val_top-5-accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.3110 - accuracy: 0.8881 - top-5-accuracy: 0.9994 - val_loss: 0.4720 - val_accuracy: 0.8497 - val_top-5-accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.2770 - accuracy: 0.9055 - top-5-accuracy: 1.0000 - val_loss: 0.2736 - val_accuracy: 0.9249 - val_top-5-accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.2803 - accuracy: 0.8952 - top-5-accuracy: 0.9994 - val_loss: 0.5448 - val_accuracy: 0.8035 - val_top-5-accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.2897 - accuracy: 0.8945 - top-5-accuracy: 1.0000 - val_loss: 0.2879 - val_accuracy: 0.9249 - val_top-5-accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.2733 - accuracy: 0.9100 - top-5-accuracy: 1.0000 - val_loss: 0.2403 - val_accuracy: 0.9249 - val_top-5-accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.2678 - accuracy: 0.9100 - top-5-accuracy: 1.0000 - val_loss: 0.3087 - val_accuracy: 0.9133 - val_top-5-accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.2128 - accuracy: 0.9299 - top-5-accuracy: 1.0000 - val_loss: 0.2315 - val_accuracy: 0.9133 - val_top-5-accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.2373 - accuracy: 0.9183 - top-5-accuracy: 1.0000 - val_loss: 0.4300 - val_accuracy: 0.8324 - val_top-5-accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.2335 - accuracy: 0.9132 - top-5-accuracy: 1.0000 - val_loss: 0.2068 - val_accuracy: 0.9480 - val_top-5-accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.2395 - accuracy: 0.9145 - top-5-accuracy: 1.0000 - val_loss: 0.3970 - val_accuracy: 0.8902 - val_top-5-accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.2132 - accuracy: 0.9260 - top-5-accuracy: 1.0000 - val_loss: 0.1306 - val_accuracy: 0.9422 - val_top-5-accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.2328 - accuracy: 0.9248 - top-5-accuracy: 1.0000 - val_loss: 0.1355 - val_accuracy: 0.9480 - val_top-5-accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "195/195 [==============================] - 6s 30ms/step - loss: 0.1839 - accuracy: 0.9350 - top-5-accuracy: 1.0000 - val_loss: 0.2881 - val_accuracy: 0.9133 - val_top-5-accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.2106 - accuracy: 0.9228 - top-5-accuracy: 1.0000 - val_loss: 0.5075 - val_accuracy: 0.8092 - val_top-5-accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.2007 - accuracy: 0.9293 - top-5-accuracy: 1.0000 - val_loss: 0.2905 - val_accuracy: 0.9191 - val_top-5-accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.1844 - accuracy: 0.9350 - top-5-accuracy: 1.0000 - val_loss: 0.2432 - val_accuracy: 0.9133 - val_top-5-accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.1928 - accuracy: 0.9357 - top-5-accuracy: 1.0000 - val_loss: 0.2657 - val_accuracy: 0.8960 - val_top-5-accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.2075 - accuracy: 0.9280 - top-5-accuracy: 1.0000 - val_loss: 0.4727 - val_accuracy: 0.8266 - val_top-5-accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.1834 - accuracy: 0.9344 - top-5-accuracy: 1.0000 - val_loss: 0.5405 - val_accuracy: 0.8439 - val_top-5-accuracy: 0.9942\n",
            "Epoch 39/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.1707 - accuracy: 0.9363 - top-5-accuracy: 0.9994 - val_loss: 0.4760 - val_accuracy: 0.8613 - val_top-5-accuracy: 0.9942\n",
            "Epoch 40/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.1851 - accuracy: 0.9331 - top-5-accuracy: 1.0000 - val_loss: 0.2211 - val_accuracy: 0.9364 - val_top-5-accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.2078 - accuracy: 0.9248 - top-5-accuracy: 1.0000 - val_loss: 0.1260 - val_accuracy: 0.9480 - val_top-5-accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.1671 - accuracy: 0.9408 - top-5-accuracy: 1.0000 - val_loss: 0.1699 - val_accuracy: 0.9364 - val_top-5-accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "195/195 [==============================] - 5s 27ms/step - loss: 0.2036 - accuracy: 0.9260 - top-5-accuracy: 1.0000 - val_loss: 0.0559 - val_accuracy: 0.9884 - val_top-5-accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.1829 - accuracy: 0.9370 - top-5-accuracy: 1.0000 - val_loss: 0.1223 - val_accuracy: 0.9711 - val_top-5-accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.1495 - accuracy: 0.9421 - top-5-accuracy: 0.9994 - val_loss: 0.1406 - val_accuracy: 0.9538 - val_top-5-accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.1501 - accuracy: 0.9531 - top-5-accuracy: 1.0000 - val_loss: 0.1782 - val_accuracy: 0.9422 - val_top-5-accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.1522 - accuracy: 0.9479 - top-5-accuracy: 0.9994 - val_loss: 0.0793 - val_accuracy: 0.9711 - val_top-5-accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.1923 - accuracy: 0.9305 - top-5-accuracy: 1.0000 - val_loss: 0.1097 - val_accuracy: 0.9538 - val_top-5-accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.1512 - accuracy: 0.9537 - top-5-accuracy: 1.0000 - val_loss: 0.1877 - val_accuracy: 0.9191 - val_top-5-accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.1453 - accuracy: 0.9550 - top-5-accuracy: 1.0000 - val_loss: 0.1732 - val_accuracy: 0.9422 - val_top-5-accuracy: 1.0000\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1259 - accuracy: 0.9444 - top-5-accuracy: 1.0000\n",
            "Test accuracy: 94.44%\n",
            "Test top 5 accuracy: 100.0%\n"
          ]
        }
      ],
      "source": [
        "# Some code is taken from:\n",
        "# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.\n",
        "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(\n",
        "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.learning_rate_base = learning_rate_base\n",
        "        self.total_steps = total_steps\n",
        "        self.warmup_learning_rate = warmup_learning_rate\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.pi = tf.constant(np.pi)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        if self.total_steps < self.warmup_steps:\n",
        "            raise ValueError(\n",
        "                \"Total_steps must be larger or equal to warmup_steps.\")\n",
        "\n",
        "        cos_annealed_lr = tf.cos(\n",
        "            self.pi\n",
        "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
        "            / float(self.total_steps - self.warmup_steps)\n",
        "        )\n",
        "        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n",
        "\n",
        "        if self.warmup_steps > 0:\n",
        "            if self.learning_rate_base < self.warmup_learning_rate:\n",
        "                raise ValueError(\n",
        "                    \"Learning_rate_base must be larger or equal to \"\n",
        "                    \"warmup_learning_rate.\"\n",
        "                )\n",
        "            slope = (\n",
        "                self.learning_rate_base - self.warmup_learning_rate\n",
        "            ) / self.warmup_steps\n",
        "            warmup_rate = slope * \\\n",
        "                tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
        "            learning_rate = tf.where(\n",
        "                step < self.warmup_steps, warmup_rate, learning_rate\n",
        "            )\n",
        "        return tf.where(\n",
        "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
        "        )\n",
        "\n",
        "\n",
        "def run_experiment(model):\n",
        "    total_steps = int((len(X_train) / BATCH_SIZE) * EPOCHS)\n",
        "    warmup_epoch_percentage = 0.10\n",
        "    warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
        "    scheduled_lrs = WarmUpCosine(\n",
        "        learning_rate_base=LEARNING_RATE,\n",
        "        total_steps=total_steps,\n",
        "        warmup_learning_rate=0.0,\n",
        "        warmup_steps=warmup_steps,\n",
        "    )\n",
        "\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(\n",
        "                5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=X_train,\n",
        "        y=y_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=0.1,\n",
        "    )\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(\n",
        "        X_test, y_test, batch_size=BATCH_SIZE)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history, model\n",
        "\n",
        "\n",
        "# Run experiments with the vanilla ViT\n",
        "vit = create_vit_classifier(vanilla=True)\n",
        "history = run_experiment(vit)\n",
        "\n",
        "# Run experiments with the Shifted Patch Tokenization and\n",
        "# Locality Self Attention modified ViT\n",
        "vit_sl = create_vit_classifier(vanilla=False)\n",
        "history, vit_sl = run_experiment(vit_sl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ixrBnxgWs5e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyl4P4DGWs5e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GYCnVP3Ws5e",
        "outputId": "c5740a25-08f7-4856-e4c2-4839f510e315"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 16ms/step\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          Cr       0.86      1.00      0.92        12\n",
            "          Sc       1.00      0.83      0.91        12\n",
            "          In       0.92      1.00      0.96        12\n",
            "          PS       1.00      0.92      0.96        12\n",
            "          RS       1.00      1.00      1.00        12\n",
            "          Pa       0.83      0.83      0.83        12\n",
            "\n",
            "    accuracy                           0.93        72\n",
            "   macro avg       0.94      0.93      0.93        72\n",
            "weighted avg       0.94      0.93      0.93        72\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x864 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "dark"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def conf_matrix(model, X_test, Y_test):\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    Y_pred = np.argmax(np.round(predictions), axis=1)\n",
        "\n",
        "    cm = confusion_matrix(Y_test, Y_pred)\n",
        "\n",
        "    print(\"Classification Report:\\n\")\n",
        "\n",
        "    cr = classification_report(Y_test,\n",
        "                               Y_pred,\n",
        "                               target_names=[class_types[i] for i in range(len(class_types))])\n",
        "    print(cr)   # change to save to file?\n",
        "\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    sns_hmp = sns.heatmap(cm, annot=True, xticklabels=[class_types[i] for i in range(len(class_types))],\n",
        "                          yticklabels=[class_types[i] for i in range(len(class_types))], fmt=\"d\")\n",
        "    fig = sns_hmp.get_figure()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "conf_matrix(vit_sl, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPioOkEEWs5e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQle3gfDWs5e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}